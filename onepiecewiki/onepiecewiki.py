# onepiecewiki.py
from __future__ import annotations

import asyncio
import re
import html as ihtml
from typing import Any, Dict, Optional, Tuple, List

import aiohttp
import discord
from redbot.core import commands, Config
from redbot.core.utils.chat_formatting import humanize_list

API_BASE = "https://onepiece.fandom.com/api.php"
WIKI_BASE = "https://onepiece.fandom.com/wiki/"

# ---------- cleaning helpers ----------
WIKILINK = re.compile(r"\[\[(?:[^|\]]+\|)?([^\]]+)\]\]")
STRIP_TAGS = re.compile(r"<[^>]+>")
WS = re.compile(r"\s+")
REFNUM = re.compile(r"(?:\[\d+\])+")          # [1] / [1][2]...
DANGLING_SEMI = re.compile(r"\s*;\s*")
DANGLING_COMMA = re.compile(r"\s*,\s*,+")
MULTI_SEP = re.compile(r"\s*[;•|\u2022]\s*")  # ; or •

def _clean_wikival(val: str) -> str:
  """Strip wiki markup, citations [1], html, and tidy punctuation."""
  val = ihtml.unescape(val or "")
  val = WIKILINK.sub(r"\1", val)
  val = REFNUM.sub("", val)
  val = STRIP_TAGS.sub("", val)
  val = DANGLING_SEMI.sub(", ", val)
  val = DANGLING_COMMA.sub(", ", val)
  val = WS.sub(" ", val).strip(" ,;")
  return val

def _norm_key(key: str) -> str:
  return re.sub(r"[^a-z0-9]+", "", (key or "").lower())

def _limit(text: str, n: int = 1024) -> str:
  return text if len(text) <= n else (text[: n - 1] + "…")

def _bullets(val: str, *, max_items: int = 12) -> str:
  if not val:
    return ""
  s = _clean_wikival(val)
  parts = [p.strip() for p in MULTI_SEP.split(s) if p.strip()]
  if len(parts) == 1 and "," in s:
    parts = [p.strip() for p in s.split(",") if p.strip()]
  if not parts:
    parts = [s]
  parts = parts[:max_items]
  return "\n".join(f"• {p}" for p in parts)

# ---------- bounty parsing (HTML-aware & robust) ----------
GROUPED_ANY = re.compile(r"\d(?:[\d,\.\u00A0\u202F ]{2,}\d)")  # numbers with grouping (commas/spaces/thin-spaces/dots)
NUM_SEP = re.compile(r"[^\d]")  # strip non-digits
NOTE_RE = re.compile(r"\(([^)]+)\)")

MIN_BOUNTY = 1_000_000
MAX_BOUNTY = 10_000_000_000_000  # 10T cap
ORDINALS = ["1st known","2nd known","3rd known","4th known","5th known",
      "6th known","7th known","8th known","9th known","10th known"]

SECTION_H2 = re.compile(r"<h2[^>]*>.*?</h2>", re.I | re.S)
STRIP_ANCHORS = re.compile(r'<a[^>]*>(.*?)</a>', re.I | re.S)
H2_BLOCK_RE = re.compile(r"(<h2[^>]*>.*?</h2>)(.*?)(?=<h2|$)", re.I | re.S)

def _fmt_amount(n: int) -> str:
  return f"{n:,}"

def _extract_bounty_amounts(text_or_html: str) -> List[int]:
  """
  Find all plausible bounty numbers inside a string/HTML:
  - accept commas, spaces, thin spaces, dots
  - strip all non-digits and parse
  - filter to >= 1,000,000 and sane upper bound
  """
  amounts: List[int] = []
  seen = set()

  clean = (text_or_html or "").replace("\u2009", " ").replace("\u202F", " ").replace("\u00A0", " ")

  for m in re.finditer(r"\d[\d,\. ]*\d", clean):
    digits = re.sub(r"[^\d]", "", m.group(0))
    if not digits:
      continue
    try:
      amt = int(digits)
    except ValueError:
      continue
    if MIN_BOUNTY <= amt <= MAX_BOUNTY and amt not in seen:
      seen.add(amt)
      amounts.append(amt)
  amounts.sort()
  return amounts

def _extract_bounties_from_html(html: str) -> List[int]:
  # Prefer the PortableInfobox data-source form
  blocks = re.findall(
    r'<div[^>]*data-source="bounty"[^>]*>.*?<div[^>]*class="[^"]*pi-data-value[^"]*"[^>]*>(.*?)</div>',
    html, re.I | re.S
  )
  if not blocks:
    # Fallback to label (Bounty / Bounties)
    blocks = re.findall(
      r'<h3[^>]*class="[^"]*pi-data-label[^"]*"[^>]*>\s*Bount?ies?\s*</h3>\s*'
      r'<div[^>]*class="[^"]*pi-data-value[^"]*"[^>]*>(.*?)</div>',
      html, re.I | re.S
    )
  if not blocks:
    return []
  return _extract_bounty_amounts(" ".join(blocks))

def _parse_bounty_text(raw: str) -> List[int]:
  return _extract_bounty_amounts(_clean_wikival(raw))

# ---------- debut extraction (HTML-aware) ----------
def _extract_debut_from_html(html: str) -> Tuple[Optional[str], Optional[str], Optional[int], Optional[int]]:
  """
  Returns (manga_text, anime_text, chapter_no, episode_no) best-effort.
  Most pages use a single "Debut" row with Manga/Anime inline items.
  """
  # Try the unified "Debut" row first
  m = re.search(
    r'<h3[^>]*class="[^"]*pi-data-label[^"]*"[^>]*>\s*Debut\s*</h3>\s*'
    r'<div[^>]*class="[^"]*pi-data-value[^"]*"[^>]*>(.*?)</div>',
    html, re.I | re.S
  )
  if not m:
    # Rare legacy: separate Manga Debut/Anime Debut rows
    def grab(label: str) -> Optional[str]:
      mm = re.search(
        rf'<h3[^>]*class="[^"]*pi-data-label[^"]*"[^>]*>\s*{label}\s*</h3>\s*'
        r'<div[^>]*class="[^"]*pi-data-value[^"]*"[^>]*>(.*?)</div>',
        html, re.I | re.S
      )
      return mm.group(1) if mm else None
    man_html = grab("Manga Debut")
    ani_html = grab("Anime Debut")
  else:
    debut_val_html = m.group(1)
    im_m = re.search(r'(?:Manga)\s*[:：]\s*(.*?)(?=$|<br|</div)', debut_val_html, re.I | re.S)
    im_a = re.search(r'(?:Anime)\s*[:：]\s*(.*?)(?=$|<br|</div)', debut_val_html, re.I | re.S)
    man_html = im_m.group(1) if im_m else None
    ani_html = im_a.group(1) if im_a else None

  man_text = _clean_wikival(man_html) if man_html else None
  ani_text = _clean_wikival(ani_html) if ani_html else None

  chap_no: Optional[int] = None
  ep_no: Optional[int] = None

  # Chapter number from HTML or cleaned text
  if man_html and chap_no is None:
    m = re.search(r"(?:Chapter[_\s]|title=\"Chapter\s*|Chapter\s+)(\d{1,4})", man_html, re.I)
    if m: chap_no = int(m.group(1))
  if chap_no is None and man_text:
    m = re.search(r"chapter\s+(\d{1,4})", man_text, re.I)
    if m: chap_no = int(m.group(1))

  # Episode number from HTML or cleaned text
  if ani_html and ep_no is None:
    m = re.search(r"(?:Episode[_\s]|title=\"Episode\s*|Episode\s+)(\d{1,4})", ani_html, re.I)
    if m: ep_no = int(m.group(1))
  if ep_no is None and ani_text:
    m = re.search(r"episode\s+(\d{1,4})", ani_text, re.I)
    if m: ep_no = int(m.group(1))

  return man_text, ani_text, chap_no, ep_no

# ---------- aliases ----------
COMMON_ALIASES: Dict[str, str] = {
  "luffy": "Monkey D. Luffy", "zoro": "Roronoa Zoro", "roronoa": "Roronoa Zoro",
  "sanji": "Vinsmoke Sanji", "nami": "Nami", "usopp": "Usopp",
  "chopper": "Tony Tony Chopper", "robin": "Nico Robin", "franky": "Franky",
  "brook": "Brook", "jinbe": "Jinbe", "jimbei": "Jinbe", "ace": "Portgas D. Ace",
  "sabo": "Sabo", "shanks": "Shanks", "buggy": "Buggy", "garp": "Monkey D. Garp",
  "kuzan": "Kuzan", "aokiji": "Kuzan", "kizaru": "Borsalino", "borsalino": "Borsalino",
  "akainu": "Sakazuki", "sakazuki": "Sakazuki", "law": "Trafalgar D. Water Law",
  "trafalgar": "Trafalgar D. Water Law", "mihawk": "Dracule Mihawk",
  "doflamingo": "Donquixote Doflamingo", "dofy": "Donquixote Doflamingo",
  "katakuri": "Charlotte Katakuri", "big mom": "Charlotte Linlin", "linlin": "Charlotte Linlin",
  "kaido": "Kaido", "kaidou": "Kaido", "yamato": "Yamato", "kid": "Eustass Kid",
  "killer": "Killer", "hancock": "Boa Hancock", "boa": "Boa Hancock",
  "smoker": "Smoker", "crocodile": "Crocodile", "eneru": "Enel", "enel": "Enel",
}

def _fuzzy_query(q: str) -> str:
  tokens = [t for t in re.split(r"\s+", q.strip()) if t]
  return " ".join((t if len(t) < 4 else f"{t}~") for t in tokens) or q

# ---------- Cog ----------
class OnePieceWiki(commands.Cog):
  """One Piece Wiki — clean card with big image, accurate bounty list, crews, and debut chapter/episode."""

  __author__ = "your_name"
  __version__ = "2.9.0"

  def __init__(self, bot):
    self.bot = bot
    self.session: Optional[aiohttp.ClientSession] = None
    self.config = Config.get_conf(self, identifier=2025091001, force_registration=True)
    self.config.register_guild(aliases={})

  async def cog_load(self):
    self.session = aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=12))

  async def cog_unload(self):
    if self.session and not self.session.closed:
      await self.session.close()

  @commands.hybrid_command(name="wiki", aliases=["opwiki"], usage="<character>")
  @commands.guild_only()
  @commands.bot_in_a_guild()
  @commands.bot_has_permissions(embed_links=True)
  @commands.cooldown(3, 10, commands.BucketType.user)
  @commands.max_concurrency(2, commands.BucketType.channel, wait=True)
  async def wiki(self, ctx: commands.Context, *, query: str):
    """Search OP Wiki and display a lean character card."""
    await ctx.typing()
    # support for the feats feature
    parts = query.split()
    mode = None
    if len(parts) >= 2:
      last = parts[-1].lower()
      if last in {"feats"}:
        mode = last
        query = " ".join(parts[:-1]).strip()

    # ===== mode: feats =====
    if mode == "feats":
      sections = self._extract_section_blocks(raw_html)
      body_html = None
      # exact title match first
      for key in self.PREFERRED_FEATS:
        body_html = sections.get(key)
        if body_html:
          break
      # fuzzy title contains as fallback
      if not body_html:
        for t, b in sections.items():
          if any(k in t for k in self.PREFERRED_FEATS):
            body_html = b
            break

      if not body_html:
        return await ctx.reply("I couldn't find a Feats/Powers section on this page.")

      bullets = self._html_to_bullets(body_html, limit=12)
      if not bullets:
        return await ctx.reply("The Feats/Powers section has no listable items.")

      display_title = info.get("name") or title
      em = discord.Embed(
        title=f"{display_title} — Feats",
        url=page_url,
        color=discord.Color.gold()
      )
      em.description = _limit("\n".join(bullets), 4000)
      if big_img:
        em.set_thumbnail(url=big_img)  # thumbnail so the list stays readable
      em.set_footer(text="From the Abilities/Powers section (best match)")
      # show suggestions (if any) under feats too
      await ctx.reply(embed=em)
      if suggestions:
        sug = humanize_list([f"[{t}]({WIKI_BASE}{t.replace(' ', '_')})" for t in suggestions[:4]])
        tiny = discord.Embed(description=f"**Did you mean:** {sug}", color=discord.Color.dark_grey())
        await ctx.send(embed=tiny)
      return

    try:
      title, suggestions = await self._search(ctx.guild, query)
      if not title:
        return await ctx.reply("No results found. Try another name or spelling.")
      info, page_url, raw_html = await self._fetch_infobox(title)
      summary, big_img = await self._fetch_summary_and_image(title)
    except asyncio.TimeoutError:
      return await ctx.reply("The wiki timed out. Try again in a moment.")
    except aiohttp.ClientError:
      return await ctx.reply("I couldn't reach the wiki right now. Please try later.")

    if summary and not info.get("summary"):
      info["summary"] = summary
    if big_img and not info.get("_image"):
      info["_image"] = big_img

    # Bounties straight from HTML (fallback to text value)
    bounty_amounts = _extract_bounties_from_html(raw_html)
    if not bounty_amounts and info.get("bounty"):
      bounty_amounts = _parse_bounty_text(info["bounty"])

    # Debut extraction from HTML (fallback to text)
    man_text_h, ani_text_h, chap_no_h, ep_no_h = _extract_debut_from_html(raw_html)
    man_text = man_text_h or _clean_wikival(self._first(info, ["mangadebut", "debutmanga"]) or "")
    ani_text = ani_text_h or _clean_wikival(self._first(info, ["animedebut", "debutanime", "firstappearance"]) or "")
    chap_no = chap_no_h if chap_no_h is not None else self._extract_first_number(man_text, "chapter")
    ep_no   = ep_no_h if ep_no_h is not None   else self._extract_first_number(ani_text, "episode")

    display_title = info.get("name") or title
    embed = discord.Embed(title=display_title, url=page_url, color=discord.Color.blurple())
    embed.set_author(name="One Piece Wiki", url=WIKI_BASE)

    # Crews (current / former)
    crews_current, crews_former = self._extract_crews(info)
    if crews_current:
      embed.add_field(name="Crews — Current", value=_limit(_bullets(crews_current)), inline=False)
    if crews_former:
      embed.add_field(name="Crews — Former", value=_limit(_bullets(crews_former)), inline=False)

    # Information (lean)
    lines: List[str] = []

    pirate_flag, marine_flag = self._infer_roles(
      info.get("affiliations"), info.get("occupation") or info.get("occupations")
    )
    if pirate_flag is not None:
      lines.append(f"**Pirate:** {'Yes 🏴‍☠️' if pirate_flag else 'No'}")

    fruit = self._first(info, ["devilfruit", "devilfruittype", "fruit"])
    haki  = self._first(info, ["haki", "hakitype", "hakitypes"])
    if fruit: lines.append(f"**Devil Fruit:** {_clean_wikival(fruit)}") # noqa
    if haki:  lines.append(f"**Haki:** {_clean_wikival(haki)}")

    if bounty_amounts and not marine_flag:
      lines.append("**Bounty:**")
      for i, amt in enumerate(bounty_amounts):
        label = ORDINALS[i] if i < len(ORDINALS) else f"{i+1}th known"
        if i == len(bounty_amounts) - 1:
          label = f"{label} • Current"
        lines.append(f"• {_fmt_amount(amt)} — {label}")

    # Debut lines
    if man_text:
      lines.append(f"**Debut (Manga):** {man_text}")
    if ani_text:
      lines.append(f"**Debut (Anime):** {ani_text}")
    if chap_no is not None:
      lines.append(f"**First Chapter:** {chap_no}")
    if ep_no is not None:
      lines.append(f"**First Episode:** {ep_no}")

    # Stats
    for label, keys in [
      ("Status",   ["status", "alive", "deceased"]),
      ("Epithet",  ["epithet", "nicknames", "alias"]),
      ("Age",      ["age"]),
      ("Birthday", ["birthday"]),
      ("Origin",   ["origin", "birthplace", "residence"]),
      ("Height",   ["height"]),
    ]:
      v = self._first(info, keys)
      if v:
        lines.append(f"**{label}:** {_clean_wikival(v)}")

    canon = info.get("_canon")
    if canon is not None:
      lines.append(f"**Canon:** {'Canon' if canon else 'Non-Canon'}")

    summ = (info.get("summary") or "").strip()
    if summ:
      lines.append("")
      lines.append(_limit(_clean_wikival(summ), 700))

    if lines:
      embed.add_field(name="Information", value=_limit("\n".join(lines).strip()), inline=False)

    if info.get("_image"):
      embed.set_image(url=info["_image"])

    embed.set_footer(text=f"Result powered by Fandom • {self.__version__}")
    await ctx.reply(embed=embed)

    if suggestions:
      sug = humanize_list([f"[{t}]({WIKI_BASE}{t.replace(' ', '_')})" for t in suggestions[:4]])
      tiny = discord.Embed(description=f"**Did you mean:** {sug}", color=discord.Color.dark_grey())
      await ctx.send(embed=tiny)

  # ---------- alias admin ----------
  @commands.hybrid_group(name="wikialias", aliases=["wikalias", "opalias"], invoke_without_command=True)
  @commands.guild_only()
  async def wikialias(self, ctx: commands.Context):
    await ctx.send_help()

  @wikialias.command(name="add")
  @commands.guild_only()
  @commands.mod_or_permissions(manage_guild=True)
  async def wikialias_add(self, ctx: commands.Context, key: str, *, target: str):
    key_l = key.strip().lower()
    async with self.config.guild(ctx.guild).aliases() as al:
      al[key_l] = target.strip()
    await ctx.reply(f"Alias **{key_l}** → **{target.strip()}** added.")

  @wikialias.command(name="remove", aliases=["del", "rm"])
  @commands.guild_only()
  @commands.mod_or_permissions(manage_guild=True)
  async def wikialias_remove(self, ctx: commands.Context, key: str):
    key_l = key.strip().lower()
    async with self.config.guild(ctx.guild).aliases() as al:
      if key_l in al:
        del al[key_l]
        return await ctx.reply(f"Alias **{key_l}** removed.")
    await ctx.reply("That alias doesn't exist here.")

  @wikialias.command(name="list")
  @commands.guild_only()
  async def wikialias_list(self, ctx: commands.Context):
    al = await self.config.guild(ctx.guild).aliases()
    if not al:
      return await ctx.reply("No aliases set for this server.")
    lines = [f"• **{k}** → {v}" for k, v in sorted(al.items())]
    await ctx.reply("**Aliases:**\n" + "\n".join(lines))

  # ---------- internals ----------
  async def _title_exists(self, title: str) -> Optional[str]:
    assert self.session is not None
    params = {"action": "query", "titles": title, "prop": "categories", "cllimit": 10,
          "format": "json", "redirects": 1, "utf8": 1}
    async with self.session.get(API_BASE, params=params) as resp:
      data = await resp.json()
    pages = data.get("query", {}).get("pages", {})
    if not pages:
      return None
    page = next(iter(pages.values()))
    if page.get("missing"):
      return None
    return page.get("title")

  async def _search(self, guild: Optional[discord.Guild], query: str) -> Tuple[Optional[str], List[str]]:
    assert self.session is not None
    q = (query or "").strip()
    if not q:
      return None, []

    guild_aliases: Dict[str, str] = await self.config.guild(guild).aliases() if guild else {}
    alias_target = guild_aliases.get(q.lower()) or COMMON_ALIASES.get(q.lower())
    if alias_target:
      exact = await self._title_exists(alias_target)
      if exact:
        return exact, []

    async def run_search(srsearch: str, limit: int = 8):
      params = {"action": "query", "list": "search", "srsearch": srsearch,
            "srlimit": limit, "format": "json", "utf8": 1}
      async with self.session.get(API_BASE, params=params) as resp:
        return await resp.json()

    data = await run_search(q, 8)
    hits = data.get("query", {}).get("search", [])
    if not hits:
      data = await run_search(_fuzzy_query(q), 8)
      hits = data.get("query", {}).get("search", [])

    if not hits:
      params_os = {"action": "opensearch", "search": q, "limit": 5,
             "namespace": 0, "redirects": "resolve", "format": "json"}
      async with self.session.get(API_BASE, params=params_os) as resp:
        osj = await resp.json()
      if len(osj) >= 2 and osj[1]:
        suggestions = list(map(str, osj[1]))
        first = await self._title_exists(suggestions[0])
        return (first or suggestions[0]), suggestions[1:4]
      return None, []

    tokens = [t for t in re.split(r"\s+", q.lower()) if t]
    def score(hit):
      title = hit.get("title", "")
      tlow = title.lower()
      base = hit.get("score", 0) if "score" in hit else 0
      token_bonus = sum(1 for t in tokens if t in tlow) * 10
      paren_bonus = 5 if any(k in tlow for k in ("(", ")")) else 0
      return base + token_bonus + paren_bonus

    hits.sort(key=score, reverse=True)
    top = hits[0]["title"]
    suggestions = [h["title"] for h in hits[1:6]]
    canonical = await self._title_exists(top)
    return (canonical or top), suggestions

  async def _fetch_summary_and_image(self, title: str) -> Tuple[str, Optional[str]]:
    assert self.session is not None
    params = {"action": "query", "prop": "extracts|pageimages", "exintro": 1, "explaintext": 1,
          "redirects": 1, "titles": title, "pithumbsize": 1200, "format": "json", "utf8": 1}
    async with self.session.get(API_BASE, params=params) as resp:
      data = await resp.json()
    pages = data.get("query", {}).get("pages", {})
    if not pages:
      return "", None
    page = next(iter(pages.values()))
    summary = _clean_wikival(page.get("extract") or "")
    img = (page.get("thumbnail") or {}).get("source")
    return summary, img

  async def _fetch_infobox(self, title: str) -> Tuple[Dict[str, str], str, str]:
    """
    Parse the PortableInfobox for key/value pairs and return:
    (info_dict, page_url, raw_html_of_the_parse)
    """
    assert self.session is not None
    page_url = f"{WIKI_BASE}{title.replace(' ', '_')}"
    params = {"action": "parse", "page": title, "prop": "text|properties|categories",
          "format": "json", "redirects": 1, "utf8": 1}
    info: Dict[str, str] = {"_canon": None}

    async with self.session.get(API_BASE, params=params) as resp:
      data = await resp.json()

    parse = data.get("parse", {})
    html = parse.get("text", {}).get("*", "") or ""
    cats = [c.get("*", "") for c in parse.get("categories", []) if isinstance(c, dict)]
    if any("Non-Canon" in c or "Non Canon" in c for c in cats):
      info["_canon"] = False

    # image
    m_img = re.search(r'<figure[^>]*class="[^"]*pi-image[^"]*"[^>]*>.*?<img[^>]+src="([^"]+)"', html, re.S)
    if m_img:
      info["_image"] = ihtml.unescape(m_img.group(1))

    # name
    m_name = re.search(r'<h2[^>]*class="[^"]*pi-title[^"]*"[^>]*>(.*?)</h2>', html, re.S)
    if m_name:
      info["name"] = _clean_wikival(m_name.group(1))

    # key/value rows
    for lab, val in re.findall(
      r'<h3[^>]*class="[^"]*pi-data-label[^"]*"[^>]*>(.*?)</h3>\s*'
      r'<div[^>]*class="[^"]*pi-data-value[^"]*"[^>]*>(.*?)</div>',
      html, re.S,
    ):
      key = _norm_key(_clean_wikival(lab))
      value = _clean_wikival(val)
      if not key or not value:
        continue
      if key in info:
        info[key] = humanize_list([info[key], value])
      else:
        info[key] = value

    if "affiliations" not in info and "affiliation" in info:
      info["affiliations"] = info["affiliation"]

    # canon explicit field overrides category inference
    canon_field = info.get("canon") or info.get("iscanon")
    if canon_field:
      fl = canon_field.strip().lower()
      if fl in {"yes", "y", "true", "canon"}:
        info["_canon"] = True
      elif fl in {"no", "n", "false", "non-canon", "non canon"}:
        info["_canon"] = False
    elif info["_canon"] is None:
      info["_canon"] = True

    return info, page_url, html

  # ---- utilities ----
  def _first(self, info: Dict[str, str], keys: List[str]) -> Optional[str]:
    for k in keys:
      nk = _norm_key(k)
      if nk in info and info[nk]:
        return info[nk]
    return None

  def _infer_roles(self, affiliations: Optional[str], occupations: Optional[str]) -> Tuple[Optional[bool], bool]:
    """Return (pirate_flag, marine_flag)."""
    text = " ".join([affiliations or "", occupations or ""]).lower()
    if not text:
      return None, False
    marine_words = ["marine", "vice admiral", "admiral", "rear admiral", "ensign", "seaman",
            "cipher pol", "world government", "cp0", "cp-0", "cp9", "navy"]
    marine_flag = any(w in text for w in marine_words)
    pirate_flag = ("pirate" in text or "pirates" in text)
    return (True if pirate_flag else (False if marine_flag else None), marine_flag)

  def _extract_crews(self, info: Dict[str, str]) -> Tuple[str, str]:
    current_sources = [
      info.get("crew"), info.get("crews"),
      info.get("piratecrew"), info.get("piratecrews"),
      info.get("affiliations"), info.get("organization"), info.get("organizations"),
    ]
    former_sources = [
      info.get("formercrew"), info.get("formercrews"),
      info.get("formeraffiliations"), info.get("previousaffiliations"), info.get("pastaffiliations"),
    ]

    cur_items: List[str] = []
    former_items: List[str] = []

    def push_items(raw: Optional[str], dest: List[str]):
      if not raw:
        return
      for p in [p.strip() for p in MULTI_SEP.split(raw) if p.strip()]:
        if p and p not in dest:
          dest.append(p)

    for src in current_sources:
      if not src:
        continue
      parts = [p.strip() for p in MULTI_SEP.split(src) if p.strip()]
      for p in parts:
        if "(former" in p.lower() or "former)" in p.lower():
          p2 = re.sub(r"\s*\(.*?former.*?\)\s*", "", p, flags=re.I).strip()
          if p2 and p2 not in former_items:
            former_items.append(p2)
        else:
          if p.lower() not in {"world government", "marines", "navy"}:
            if p not in cur_items:
              cur_items.append(p)

    for src in former_sources:
      push_items(src, former_items)

    cur_set: List[str] = []
    for x in cur_items:
      if x not in former_items and x not in cur_set:
        cur_set.append(x)

    return (", ".join(cur_set), ", ".join(former_items))

  def _extract_first_number(self, text: Optional[str], hint: str) -> Optional[int]:
    if not text:
      return None
    t = _clean_wikival(text).lower()
    if hint == "chapter":
      m = re.search(r"chapter\s+(\d{1,4})", t, re.I)
      if m:
        return int(m.group(1))
    if hint == "episode":
      m = re.search(r"episode\s+(\d{1,4})", t, re.I)
      if m:
        return int(m.group(1))
    m = re.search(r"\b(\d{1,4})\b", t)
    return int(m.group(1)) if m else None

  def _clean_html_text(self, html: str) -> str:
    return _clean_wikival(html)

  def _best_section_title(self, title_html: str) -> str:
    # Remove the anchor spans and tags to get plain text
    title_html = STRIP_ANCHORS.sub("", title_html)
    return self._clean_html_text(title_html)

  def _extract_section_blocks(self, raw_html: str) -> Dict[str, str]:
    """Return {section_title: section_html_block} for all h2 sections"""
    out = {}
    for m in H2_BLOCK_RE.finditer(raw_html):
      title_html, body_html = m.group(1), m.group(2)
      # get human title
      t_m = SECTION_H2.search(title_html)
      if not t_m:
        continue
      title = self._best_section_title(t_m.group(0))
      if not title:
        continue
      out[title] = body_html
    return out

  def _html_to_bullets(self, section_html: str, limit: int = 12) -> List[str]:
    """Pull <li> items first;  if none, split paragraphs"""
    items = re.findall(r"<li[^>]*>(.*?)</li>", section_html, re.I | re.S)
    if not items:
      paras = [p.strip() for p in re.findall(r"<p[^>]*>(.*?)</p>", section_html, re.I | re.S) if p.strip()]
      items = [p for p in paras if len(self._clean_html_text(p)) > 15]  # keep meaningful text
    bullets = []
    for it in items:
      text = self._clean_html_text(it)
      if text:
        bullets.append(f"• {text}")
      if len(bullets) >= limit:
        break
    return bullets

  PREFERRED_FEATS = [
    "abilities and powers",
    "abilities & powers",
    "powers and abilities",
    "techniques",
    "abilities",
    "powers",
  ]


# ----- Red setup (single-file cog) -----
async def setup(bot):
  await bot.add_cog(OnePieceWiki(bot))
